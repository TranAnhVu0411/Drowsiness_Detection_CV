{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-23T09:38:39.004178Z","iopub.status.busy":"2023-09-23T09:38:39.003711Z","iopub.status.idle":"2023-09-23T09:39:27.131265Z","shell.execute_reply":"2023-09-23T09:39:27.130175Z","shell.execute_reply.started":"2023-09-23T09:38:39.004141Z"},"trusted":true},"outputs":[],"source":["!pip install mediapipe\n","!pip -q install facenet-pytorch\n","!pip install decord"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import mediapipe as mp\n","import itertools\n","import cv2\n","import json\n","import os\n","import numpy as np\n","from tqdm import tqdm\n","import torch\n","from torchvision.models import resnet50, ResNet50_Weights, vgg16, VGG16_Weights\n","from torch import nn\n","from torch.utils.data import Dataset,DataLoader\n","from facenet_pytorch import InceptionResnetV1\n","import gc\n","import shutil"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-23T09:39:40.869693Z","iopub.status.busy":"2023-09-23T09:39:40.868971Z","iopub.status.idle":"2023-09-23T09:39:40.899325Z","shell.execute_reply":"2023-09-23T09:39:40.897571Z","shell.execute_reply.started":"2023-09-23T09:39:40.869623Z"},"trusted":true},"outputs":[],"source":["from decord import VideoReader"]},{"cell_type":"markdown","metadata":{},"source":["# Util"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-23T09:39:40.904541Z","iopub.status.busy":"2023-09-23T09:39:40.904071Z","iopub.status.idle":"2023-09-23T09:39:40.913296Z","shell.execute_reply":"2023-09-23T09:39:40.911858Z","shell.execute_reply.started":"2023-09-23T09:39:40.904501Z"},"trusted":true},"outputs":[],"source":["CFG = {\n","    'fold_num': 5,\n","    'vgg_embedding_features': 8631,\n","    'vgg-input-shape': 224,\n","    'n_frames': 30,\n","    'device': torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-23T09:39:40.916144Z","iopub.status.busy":"2023-09-23T09:39:40.915049Z","iopub.status.idle":"2023-09-23T09:39:40.961676Z","shell.execute_reply":"2023-09-23T09:39:40.960321Z","shell.execute_reply.started":"2023-09-23T09:39:40.916104Z"},"trusted":true},"outputs":[],"source":["# Initialize mediapipe\n","mp_face_mesh = mp.solutions.face_mesh\n","face_mesh = mp_face_mesh.FaceMesh(\n","                static_image_mode=True,\n","                max_num_faces=1,\n","                refine_landmarks=True,\n","                min_detection_confidence=0.5)\n","normalize_face_mesh = mp_face_mesh.FaceMesh(\n","                static_image_mode=True,\n","                max_num_faces=1,\n","                refine_landmarks=True,\n","                min_detection_confidence=0.5)\n","LEFT_EYE_INDEXES = list(set(itertools.chain(*mp_face_mesh.FACEMESH_LEFT_EYE)))\n","RIGHT_EYE_INDEXES = list(set(itertools.chain(*mp_face_mesh.FACEMESH_RIGHT_EYE)))\n","LIP_INDEXES = list(set(itertools.chain(*mp_face_mesh.FACEMESH_LIPS)))\n","FACE_INDEXES = list(set(itertools.chain(*mp_face_mesh.FACEMESH_FACE_OVAL)))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-23T09:39:40.965102Z","iopub.status.busy":"2023-09-23T09:39:40.963573Z","iopub.status.idle":"2023-09-23T09:39:43.830708Z","shell.execute_reply":"2023-09-23T09:39:43.829168Z","shell.execute_reply.started":"2023-09-23T09:39:40.965061Z"},"trusted":true},"outputs":[],"source":["# Initialize VGG model\n","resnet = InceptionResnetV1(pretrained = 'vggface2', classify=True).to(CFG['device']).eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-23T09:39:43.832401Z","iopub.status.busy":"2023-09-23T09:39:43.832036Z","iopub.status.idle":"2023-09-23T09:39:43.844252Z","shell.execute_reply":"2023-09-23T09:39:43.842950Z","shell.execute_reply.started":"2023-09-23T09:39:43.832369Z"},"trusted":true},"outputs":[],"source":["# Padding feature\n","def pad(array, target_shape):\n","    if array.shape[0]==target_shape[0]:\n","        return array\n","    else:\n","        padding = torch.zeros((target_shape[0]-array.shape[0], *target_shape[1:]))\n","        return torch.concatenate([array.to(CFG['device']), padding.to(CFG['device'])], axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-23T09:39:43.846334Z","iopub.status.busy":"2023-09-23T09:39:43.845922Z","iopub.status.idle":"2023-09-23T09:39:43.859975Z","shell.execute_reply":"2023-09-23T09:39:43.858698Z","shell.execute_reply.started":"2023-09-23T09:39:43.846300Z"},"trusted":true},"outputs":[],"source":["# Crop images and extract keypoint\n","def crop_and_extract(h, w, frame, INDEXES, result):\n","    cx_min=  w\n","    cy_min = h\n","    cx_max= cy_max= 0\n","    kpts = []\n","    for point in INDEXES:\n","        mediapipe_x = result.landmark[point].x\n","        mediapipe_y = result.landmark[point].y\n","        mediapipe_z = result.landmark[point].z\n","        kpts.append(mediapipe_x)\n","        kpts.append(mediapipe_y)\n","        kpts.append(mediapipe_z)\n","        cx, cy = int(mediapipe_x * w), int(mediapipe_y * h)\n","        if cx<cx_min:\n","            cx_min=cx\n","        if cy<cy_min:\n","            cy_min=cy\n","        if cx>cx_max:\n","            cx_max=cx\n","        if cy>cy_max:\n","            cy_max=cy\n","    crop = frame[cy_min : cy_max, cx_min : cx_max]\n","    return kpts, crop, (cx_min, cy_min), (cx_max, cy_max)\n","\n","# Crop images and extract keypoint\n","def keypoint_preprocess(frame_h, frame_w, top_left, bottom_right, INDEXES, result):\n","    kpts = []\n","    for point in INDEXES:\n","        mediapipe_x = result.landmark[point].x\n","        mediapipe_y = result.landmark[point].y\n","        preprocess_x = (mediapipe_x*frame_w - top_left[0])/(bottom_right[0]-top_left[0])\n","        preprocess_y = (mediapipe_y*frame_h - top_left[1])/(bottom_right[1]-top_left[1])\n","        kpts.append(preprocess_x)\n","        kpts.append(preprocess_y)\n","    return kpts"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-23T09:39:43.862797Z","iopub.status.busy":"2023-09-23T09:39:43.862304Z","iopub.status.idle":"2023-09-23T09:39:43.890668Z","shell.execute_reply":"2023-09-23T09:39:43.889652Z","shell.execute_reply.started":"2023-09-23T09:39:43.862760Z"},"trusted":true},"outputs":[],"source":["# Load data dir metadata\n","with open('/kaggle/input/sust-ddd-metadata/dataset_metadata.json', 'r') as f:\n","    metadata = json.load(f)"]},{"cell_type":"markdown","metadata":{},"source":["# Feature Extraction (Old)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-23T08:20:18.543348Z","iopub.status.busy":"2023-09-23T08:20:18.543044Z","iopub.status.idle":"2023-09-23T08:20:18.567896Z","shell.execute_reply":"2023-09-23T08:20:18.566883Z","shell.execute_reply.started":"2023-09-23T08:20:18.543321Z"},"trusted":true},"outputs":[],"source":["class MyDataset(Dataset):\n","    def __init__(self, metadata, fold,\n","                 data_root=None,\n","                ):\n","        super().__init__()\n","        videos = []\n","        labels = []\n","        label_names = []\n","        for fold_num in fold:\n","            for label, video_names in metadata[fold_num].items():\n","                videos += video_names\n","                labels += (np.ones(len(video_names)) if label=='drowsiness' else np.zeros(len(video_names))).astype(int).tolist()\n","                label_names += [label]*len(video_names)\n","                \n","        self.data_root = data_root\n","        self.videos = videos\n","        self.labels = labels\n","        self.label_names = label_names\n","\n","    def __len__(self):\n","        return len(self.videos)\n","\n","    def __getitem__(self, index: int):\n","\n","        # get labels\n","        label = self.labels[index]\n","        label_name = self.label_names[index]\n","        video_name = self.videos[index]\n","        \n","        vd_dir = os.path.join(self.data_root, label_name, video_name)\n","\n","        # Loop through frames\n","        vr = VideoReader(vd_dir)\n","        total_frames = len(vr)\n","        sample_rate = int(total_frames/CFG['n_frames'])\n","        # Loop through frames\n","        # Keypoints\n","        lip_feats = []\n","        l_eye_feats = []\n","        r_eye_feats = []\n","        # Images\n","        faces = []\n","#         l_eyes = []\n","#         r_eyes = []\n","#         lips = []\n","        count = 0\n","        for fno in (range(0, total_frames, sample_rate)):\n","            frame = cv2.cvtColor(vr[fno].asnumpy(), cv2.COLOR_BGR2RGB)\n","#             try:\n","            results = face_mesh.process(frame).multi_face_landmarks\n","#             except:\n","#                 continue\n","            if results is not None:\n","                result = results[0]\n","                h, w, _ = frame.shape\n","                _, face, top_left, bottom_right = crop_and_extract(h, w, frame, FACE_INDEXES, result)\n","                try:\n","#                     normalize_result = normalize_face_mesh.process(face).multi_face_landmarks[0]\n","                    face = cv2.resize(face, dsize=(CFG['vgg-input-shape'], CFG['vgg-input-shape']), interpolation=cv2.INTER_CUBIC)\n","                except:\n","                    continue\n","                face = np.transpose(face, (2, 0, 1))\n","                faces.append(face)\n","                \n","#                 fl_eye_kpt, fl_eye = crop_and_extract(h, w, frame, LEFT_EYE_INDEXES, result)\n","                fl_eye_kpt = keypoint_preprocess(h, w, top_left, bottom_right, LEFT_EYE_INDEXES, result)\n","#                 fl_eye = cv2.resize(fl_eye, dsize=(CFG['vgg-input-shape'], CFG['vgg-input-shape']), interpolation=cv2.INTER_CUBIC)\n","#                 fl_eye = np.transpose(fl_eye, (2, 0, 1))\n","#                 l_eyes.append(fl_eye)\n","                l_eye_feats.append(fl_eye_kpt)\n","\n","#                 fr_eye_kpt, fr_eye = crop_and_extract(h, w, frame, RIGHT_EYE_INDEXES, result)\n","                fr_eye_kpt = keypoint_preprocess(h, w, top_left, bottom_right, RIGHT_EYE_INDEXES, result)\n","#                 fr_eye = cv2.resize(fr_eye, dsize=(CFG['vgg-input-shape'], CFG['vgg-input-shape']), interpolation=cv2.INTER_CUBIC)\n","#                 fr_eye = np.transpose(fr_eye, (2, 0, 1))\n","#                 r_eyes.append(fr_eye)\n","                r_eye_feats.append(fr_eye_kpt)\n","\n","#                 f_lip_kpt, f_lip = crop_and_extract(h, w, frame, LIP_INDEXES, result)\n","                f_lip_kpt = keypoint_preprocess(h, w, top_left, bottom_right, LIP_INDEXES, result)\n","#                 f_lip = cv2.resize(f_lip, dsize=(CFG['vgg-input-shape'], CFG['vgg-input-shape']), interpolation=cv2.INTER_CUBIC)\n","#                 f_lip = np.transpose(f_lip, (2, 0, 1))\n","#                 lips.append(f_lip)\n","                lip_feats.append(f_lip_kpt)             \n","                \n","        if len(faces)!=0:\n","            seq_len = len(faces)\n","            with torch.no_grad():\n","                inputs = torch.tensor(np.array(faces)).float().to(CFG['device'])\n","                face_feats = resnet(inputs)\n","            # Keypoints\n","            lip_feats = torch.tensor(lip_feats).float().to(CFG['device'])\n","            l_eye_feats = torch.tensor(l_eye_feats).float().to(CFG['device'])\n","            r_eye_feats = torch.tensor(r_eye_feats).float().to(CFG['device'])\n","            # Images\n","#             faces = torch.tensor(np.array(faces)).float().to(CFG['device'])\n","#             l_eyes = torch.tensor(np.array(l_eyes)).float().to(CFG['device'])\n","#             r_eyes = torch.tensor(np.array(r_eyes)).float().to(CFG['device'])\n","#             lips = torch.tensor(np.array(lips)).float().to(CFG['device'])\n","            \n","            if seq_len<CFG['n_frames']:\n","                # Keypoints\n","                lip_feats = pad(lip_feats, (CFG['n_frames'], lip_feats.shape[-1]))\n","                l_eye_feats = pad(l_eye_feats, (CFG['n_frames'], l_eye_feats.shape[-1]))\n","                r_eye_feats = pad(r_eye_feats, (CFG['n_frames'], r_eye_feats.shape[-1]))\n","                # Images   \n","                face_feats = pad(face_feats, (CFG['n_frames'], face_feats.shape[-1]))\n","#                 faces = pad(faces, (CFG['n_frames'], 3, CFG['vgg-input-shape'], CFG['vgg-input-shape']))\n","#                 l_eyes = pad(l_eyes, (CFG['n_frames'], 3, CFG['vgg-input-shape'], CFG['vgg-input-shape']))\n","#                 r_eyes = pad(r_eyes, (CFG['n_frames'], 3, CFG['vgg-input-shape'], CFG['vgg-input-shape']))\n","#                 lips = pad(lips, (CFG['n_frames'], 3, CFG['vgg-input-shape'], CFG['vgg-input-shape']))\n","            else:\n","                # Keypoints\n","                lip_feats = lip_feats[:CFG['n_frames'], :]\n","                l_eye_feats = l_eye_feats[:CFG['n_frames'], :]\n","                r_eye_feats = r_eye_feats[:CFG['n_frames'], :]\n","                # Images\n","                face_feats = face_feats[:CFG['n_frames'], :]\n","#                 faces = faces[:CFG['n_frames'], :]\n","#                 l_eyes = l_eyes[:CFG['n_frames'], :]\n","#                 r_eyes = r_eyes[:CFG['n_frames'], :]\n","#                 lips = lips[:CFG['n_frames'], :]\n","                seq_len = CFG['n_frames']\n","            return video_name, (face_feats, lip_feats, l_eye_feats, r_eye_feats), seq_len, label   \n","\n","#             return video_name, (faces, lips, l_eyes, r_eyes, lip_feats, l_eye_feats, r_eye_feats), seq_len, label   \n","        else:\n","            return video_name, None, 0, label"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-23T08:20:18.570115Z","iopub.status.busy":"2023-09-23T08:20:18.569774Z","iopub.status.idle":"2023-09-23T08:20:18.585081Z","shell.execute_reply":"2023-09-23T08:20:18.584072Z","shell.execute_reply.started":"2023-09-23T08:20:18.570083Z"},"trusted":true},"outputs":[],"source":["def save_data(name, save_path='/kaggle/working/data'):\n","    if not os.path.exists(os.path.join(save_path, name)):\n","        os.makedirs(os.path.join(save_path, name))\n","    dataset = MyDataset(metadata=metadata, fold=[name], data_root='/kaggle/input/sust-ddd/SUST Driver Drowsiness Dataset')\n","    vid_names = []\n","    sequence_len = []\n","    X_face = []\n","#     X_face_img = []\n","#     X_lip_img = []\n","#     X_l_eye_img = []\n","#     X_r_eye_img = []\n","    X_lip = []\n","    X_l_eye = []\n","    X_r_eye = []\n","    Y = []\n","    for i in tqdm(range(dataset.__len__()), total = dataset.__len__()):\n","        vid_name, data, seq_len, label = dataset[i]\n","        if data!=None:\n","            # Image\n","            X_face.append(torch.unsqueeze(data[0], 0))\n","#             X_face_img.append(torch.unsqueeze(data[0], 0))\n","#             X_lip_img.append(torch.unsqueeze(data[1], 0))\n","#             X_l_eye_img.append(torch.unsqueeze(data[2], 0))\n","#             X_r_eye_img.append(torch.unsqueeze(data[3], 0))\n","            # Keypoint\n","            X_lip.append(torch.unsqueeze(data[1], 0))\n","            X_l_eye.append(torch.unsqueeze(data[2], 0))\n","            X_r_eye.append(torch.unsqueeze(data[3], 0))\n","            # Other\n","            Y.append(label)\n","            vid_names.append(vid_name)\n","            sequence_len.append(seq_len)\n","            \n","    torch.save(torch.concatenate(X_face, axis=0), os.path.join(save_path, name, 'X_face.pt'))  \n","#     torch.save(torch.concatenate(X_face_img, axis=0), os.path.join(save_path, name, 'X_face_img.pt'))\n","#     torch.save(torch.concatenate(X_lip_img, axis=0), os.path.join(save_path, name, 'X_lip_img.pt'))\n","#     torch.save(torch.concatenate(X_l_eye_img, axis=0), os.path.join(save_path, name, 'X_l_eye_img.pt'))\n","#     torch.save(torch.concatenate(X_r_eye_img, axis=0), os.path.join(save_path, name, 'X_r_eye_img.pt'))\n","    \n","    torch.save(torch.concatenate(X_lip, axis=0), os.path.join(save_path, name, 'X_lip.pt'))\n","    torch.save(torch.concatenate(X_l_eye, axis=0), os.path.join(save_path, name, 'X_l_eye.pt'))\n","    torch.save(torch.concatenate(X_r_eye, axis=0), os.path.join(save_path, name, 'X_r_eye.pt'))\n","    \n","    with open(os.path.join(save_path, name, 'vid_names.json'), 'w') as f:\n","        json.dump(vid_names, f)\n","    torch.save(torch.tensor(sequence_len), os.path.join(save_path, name, 'seq_len.pt'))\n","    torch.save(torch.tensor(Y), os.path.join(save_path, name, 'Y.pt'))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-23T08:20:18.586837Z","iopub.status.busy":"2023-09-23T08:20:18.586477Z","iopub.status.idle":"2023-09-23T09:14:15.817544Z","shell.execute_reply":"2023-09-23T09:14:15.816475Z","shell.execute_reply.started":"2023-09-23T08:20:18.586804Z"},"trusted":true},"outputs":[],"source":["save_data('fold1')\n","save_data('fold2')\n","save_data('fold3')\n","save_data('fold4')\n","save_data('fold5')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-23T09:14:15.819684Z","iopub.status.busy":"2023-09-23T09:14:15.819301Z","iopub.status.idle":"2023-09-23T09:16:06.350984Z","shell.execute_reply":"2023-09-23T09:16:06.349863Z","shell.execute_reply.started":"2023-09-23T09:14:15.819648Z"},"trusted":true},"outputs":[],"source":["shutil.make_archive('data_small', 'zip', '/kaggle/working/data')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-23T09:16:06.353162Z","iopub.status.busy":"2023-09-23T09:16:06.352650Z","iopub.status.idle":"2023-09-23T09:16:06.359222Z","shell.execute_reply":"2023-09-23T09:16:06.358137Z","shell.execute_reply.started":"2023-09-23T09:16:06.353122Z"},"trusted":true},"outputs":[],"source":["import os\n","os.chdir(r'/kaggle/working')\n","from IPython.display import FileLink"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-23T09:16:06.361335Z","iopub.status.busy":"2023-09-23T09:16:06.360980Z","iopub.status.idle":"2023-09-23T09:16:06.382928Z","shell.execute_reply":"2023-09-23T09:16:06.381885Z","shell.execute_reply.started":"2023-09-23T09:16:06.361304Z"},"trusted":true},"outputs":[],"source":["FileLink(r'data_small.zip')"]},{"cell_type":"markdown","metadata":{},"source":["# Feature Extraction (New)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-23T09:39:43.894136Z","iopub.status.busy":"2023-09-23T09:39:43.893811Z","iopub.status.idle":"2023-09-23T09:39:43.913144Z","shell.execute_reply":"2023-09-23T09:39:43.912279Z","shell.execute_reply.started":"2023-09-23T09:39:43.894108Z"},"trusted":true},"outputs":[],"source":["# class MyDataset(Dataset):\n","#     def __init__(self, metadata, fold,\n","#                  data_root=None,\n","#                 ):\n","#         super().__init__()\n","#         videos = []\n","#         labels = []\n","#         label_names = []\n","#         for fold_num in fold:\n","#             for label, video_names in metadata[fold_num].items():\n","#                 videos += video_names\n","#                 labels += (np.ones(len(video_names)) if label=='drowsiness' else np.zeros(len(video_names))).astype(int).tolist()\n","#                 label_names += [label]*len(video_names)\n","                \n","#         self.data_root = data_root\n","#         self.videos = videos\n","#         self.labels = labels\n","#         self.label_names = label_names\n","\n","#     def __len__(self):\n","#         return len(self.videos)\n","\n","#     def __getitem__(self, index: int):\n","\n","#         # get labels\n","#         label = self.labels[index]\n","#         label_name = self.label_names[index]\n","#         video_name = self.videos[index]\n","        \n","#         vd_dir = os.path.join(self.data_root, label_name, video_name)\n","\n","#         # Loop through frames\n","#         vr = VideoReader(vd_dir)\n","#         total_frames = len(vr)\n","#         sample_rate = int(total_frames/CFG['n_frames'])\n","#         # Loop through frames\n","#         # Keypoints\n","#         lip_feats = []\n","#         l_eye_feats = []\n","#         r_eye_feats = []\n","#         # Images\n","#         faces = []\n","#         count = 0\n","#         for fno in (range(0, total_frames, sample_rate)):\n","#             frame = cv2.cvtColor(vr[fno].asnumpy(), cv2.COLOR_BGR2RGB)\n","#             results = face_mesh.process(frame).multi_face_landmarks\n","#             if results is not None:\n","#                 result = results[0]\n","#                 h, w, _ = frame.shape\n","#                 _, face, top_left, bottom_right = crop_and_extract(h, w, frame, FACE_INDEXES, result)\n","#                 try:\n","#                     face = cv2.resize(face, dsize=(CFG['vgg-input-shape'], CFG['vgg-input-shape']), interpolation=cv2.INTER_CUBIC)\n","#                 except:\n","#                     continue\n","#                 face = np.transpose(face, (2, 0, 1))\n","#                 faces.append(face)\n","                \n","#                 fl_eye_kpt = keypoint_preprocess(h, w, top_left, bottom_right, LEFT_EYE_INDEXES, result)\n","#                 l_eye_feats.append(fl_eye_kpt)\n","\n","#                 fr_eye_kpt = keypoint_preprocess(h, w, top_left, bottom_right, RIGHT_EYE_INDEXES, result)\n","#                 r_eye_feats.append(fr_eye_kpt)\n","\n","#                 f_lip_kpt = keypoint_preprocess(h, w, top_left, bottom_right, LIP_INDEXES, result)\n","#                 lip_feats.append(f_lip_kpt)             \n","                \n","#         if len(faces)!=0:\n","#             faces = np.array(faces)\n","#             lip_feats = np.array(lip_feats)\n","#             l_eye_feats = np.array(l_eye_feats)\n","#             r_eye_feats = np.array(r_eye_feats)\n","#             return video_name, (faces, lip_feats, l_eye_feats, r_eye_feats)   \n","#         else:\n","#             return video_name, None"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-23T09:49:14.067286Z","iopub.status.busy":"2023-09-23T09:49:14.066797Z","iopub.status.idle":"2023-09-23T09:49:14.079290Z","shell.execute_reply":"2023-09-23T09:49:14.077600Z","shell.execute_reply.started":"2023-09-23T09:49:14.067242Z"},"trusted":true},"outputs":[],"source":["# def save_data(name, save_dir='/kaggle/working/data'):\n","#     if not os.path.exists(os.path.join(save_dir, name)):\n","#         os.makedirs(os.path.join(save_dir, name))\n","#     dataset = MyDataset(metadata=metadata, fold=[name], data_root='/kaggle/input/sust-ddd/SUST Driver Drowsiness Dataset')\n","#     for i in tqdm(range(dataset.__len__()), total = dataset.__len__()):\n","#         vid_name, data = dataset[i]\n","#         if data!=None:\n","#             save_path = os.path.join(save_dir, name, vid_name)\n","#             os.makedirs(save_path)\n","#             np.save(os.path.join(save_path, 'face.npy'),data[0])\n","#             np.save(os.path.join(save_path, 'lip.npy'),data[1])\n","#             np.save(os.path.join(save_path, 'l_eye.npy'),data[2])\n","#             np.save(os.path.join(save_path, 'r_eye.npy'),data[3])\n","#             del data\n","#             gc.collect()\n","#     gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-23T09:53:51.843113Z","iopub.status.busy":"2023-09-23T09:53:51.842226Z"},"trusted":true},"outputs":[],"source":["# print('fold1')\n","# save_data('fold1')\n","\n","# print('fold2')\n","# save_data('fold2')\n","\n","# print('fold3')\n","# save_data('fold3')\n","\n","# print('fold4')\n","# save_data('fold4')\n","\n","# print('fold5')\n","# save_data('fold5')"]},{"cell_type":"markdown","metadata":{},"source":["# Example mediapipe"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-16T03:12:01.751496Z","iopub.status.busy":"2023-09-16T03:12:01.751051Z","iopub.status.idle":"2023-09-16T03:12:01.815606Z","shell.execute_reply":"2023-09-16T03:12:01.814113Z","shell.execute_reply.started":"2023-09-16T03:12:01.751463Z"},"trusted":true},"outputs":[],"source":["# img_path = '/kaggle/input/human-faces-object-detection/images/00000074.jpg'\n","# img = cv2.imread(img_path)\n","# result = face_mesh.process(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)).multi_face_landmarks[0]\n","\n","# fl_eye_feats = []\n","# fr_eye_feats = []\n","# f_lip_feats = []\n","\n","# for point in LEFT_EYE_INDEXES:\n","#     mediapipe_x = result.landmark[point].x\n","#     mediapipe_y = result.landmark[point].y\n","#     fl_eye_feats.append(mediapipe_x)\n","\n","# for point in RIGHT_EYE_INDEXES:\n","#     mediapipe_x = result.landmark[point].x\n","#     mediapipe_y = result.landmark[point].y\n","#     fr_eye_feats.append(mediapipe_x)\n","\n","\n","# for point in LIP_INDEXES:\n","#     mediapipe_x = result.landmark[point].x\n","#     mediapipe_y = result.landmark[point].y\n","#     f_lip_feats.append(mediapipe_x)\n","\n","# print(len(LEFT_EYE_INDEXES), len(RIGHT_EYE_INDEXES), len(LIP_INDEXES))\n","# print(len(fl_eye_feats), len(fr_eye_feats), len(f_lip_feats))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-13T07:39:04.593512Z","iopub.status.busy":"2023-09-13T07:39:04.593087Z","iopub.status.idle":"2023-09-13T07:39:05.115814Z","shell.execute_reply":"2023-09-13T07:39:05.114470Z","shell.execute_reply.started":"2023-09-13T07:39:04.593475Z"},"trusted":true},"outputs":[],"source":["# # Create a MediaPipe `Pose` object\n","# with mp_pose.Pose(static_image_mode=True,\n","#                   model_complexity=2,\n","#                   enable_segmentation=True) as pose:\n","        \n","#     # Read the file in and get dims\n","#     image = cv2.imread(img_path)\n","\n","#     # Convert the BGR image to RGB and then process with the `Pose` object.\n","#     results = pose.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
